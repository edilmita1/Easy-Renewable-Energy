# -*- coding: utf-8 -*-
"""capstoneActual.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UMWip0CBEaXQBxXySizRkkBMzNGq8vFQ

## Edilma G. Fields
## Master of Science in Data Science, Grand Canyon University
## UNI – 580: Designing and Creating Data Products
## October 12, 2022



## Generate an application to develop and optimize an approach to allow the USA to project/predict having 90% of the country's energy generated predominantly by renewable energy systems.



##Developing the infrastructure necessary that intensify the implementation of renewable energies, has become an addition to the Energy problem, not a solution since all implementations have been limited to infrastructure and not to the implementation of the development of efficient transmitters, collectors, batteries, or the optimization of mechanisms to achieve the effectiveness of the current system of electrical dependency with other factors like:
##According to new research… (Local Renewable Energy Benefits and Resources, 2022).	
###1)	Change the infrastructure of the power companies in the United States!
###2)	Efficiency and Reliability of the new power system
###3)	Space and Environment Protection 
###4)	Evaluate the accessibility of regional renewable resources
###5)	Engage investors, especially regarding siting.

##Import necesary lybraries
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import seaborn as sns
sns.set(style="darkgrid")
from time import time
from collections import Counter
import seaborn as sb
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA 
from sklearn.metrics import *   
from sklearn.ensemble import RandomForestClassifier 
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KernelDensity
from sklearn.model_selection import GridSearchCV
from sklearn.compose import make_column_transformer
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler
from sklearn.utils import resample
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.decomposition import PCA
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import OneHotEncoder

# Import 'GridSearchCV', 'make_scorer', and any other necessary libraries
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer
from sklearn.metrics import fbeta_score
from sklearn.metrics import accuracy_score

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA  

import os
import sys
# %matplotlib inline

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

# Pretty display for notebooks
# %matplotlib inline

"""###Read the CSV file and display the first 5 Rows"""

allmydata = pd.read_csv('/content/total2022.csv')
allmydata.head(5)

"""Checking some information to proof the data import."""

allmydata.shape

"""The user selected the state for the projec. in this case is California 'CA'"""

mydata = allmydata[allmydata['STATE'] == 'CA']
 
print(mydata)

"""Data Confirmation information"""

mydata.shape

"""Check the data information we have integer 3 columns and 3 String columns"""

print(mydata.info())

"""Rename the columns names"""

mydata.columns = ['YEAR','MONTH','STATE','PRODUCER','ENERGY','GENERATION']
print(mydata.info())

"""# Data Clean

Lost data can happen once no data is given for some items or for an entire component. Missing data is a extremely huge problem in a real-life situations.

In order to find missing values we used isna() function and sum to count the NaN values
"""

mydata.isna().sum()

mydata['YEAR'].value_counts()

mydata['STATE'].value_counts()

mydata['PRODUCER'].value_counts()

mydata = mydata.replace('Electric Generators, Electric Utilities',1)
mydata = mydata.replace('Combined Heat and Power, Industrial Power', 2)
mydata = mydata.replace('Electric Generators, Independent Power Producers',3)
mydata = mydata.replace('Combined Heat and Power, Commercial Power',4)
mydata = mydata.replace('Combined Heat and Power, Electric Power',5)
mydata = mydata.replace('Total Electric Power Industry',6)

mydata['PRODUCER'].value_counts()

mydata['ENERGY'].value_counts()

"""### Remove irrelevants columns """

mydata.pop('YEAR')
mydata.pop('STATE')

"""Print the data information after remove the columns"""

print(mydata.info())

"""### Replace numerical value with string
### Replace all values from the Energy column that equal a specific variable
"""

mydata = mydata.replace('Natural Gas',1)
mydata = mydata.replace('Petroleum', 2)
mydata = mydata.replace('Coal',3)
mydata = mydata.replace('Other Biomass',4)
mydata = mydata.replace('Hydroelectric Conventional',5)
mydata = mydata.replace('Wood and Wood Derived Fuels',6)
mydata = mydata.replace('Other', 7)
mydata = mydata.replace('Wind',8)
mydata = mydata.replace('Other Gases', 9)
mydata = mydata.replace('Nuclear', 10)
mydata = mydata.replace('Solar Thermal and Photovoltaic', 11)
mydata = mydata.replace('Pumped Storage', 12)
mydata = mydata.replace('Geothermal', 13)
mydata = mydata.replace('Total',0)

mydata['ENERGY'].value_counts()

"""Drop a row or observation by condition in this case "total" - 0"""

print(mydata)

print(mydata.info())

import matplotlib.pyplot as plt

from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest
from pandas import read_csv
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import Pipeline
from sklearn.pipeline import FeatureUnion

"""Function to calculate IOR for each numerical attribute and show skewness"""

def detection_outlier(dfcolumn):
#Calculate Q1, Q3 and IQR
 Q1 = dfcolumn.quantile(0.25)
 Q3 = dfcolumn.quantile(0.75)
 IQR = Q3-Q1 #Interquartile range
#In general, constant should be 1.
 lower  = Q1-1*IQR
 higher = Q3+1*IQR
#Find number of outliers for specific column
 print('Before data preprocessing:')
 print('Skewness:',dfcolumn.skew())
 print(dfcolumn.describe())
 dfcolumn.loc[(dfcolumn> higher) | (dfcolumn< lower)]=dfcolumn.mean()
 print('After replacing outliers by mean:')
 print('Skewness:',dfcolumn.skew())
 print('Median:',dfcolumn.median())
 print('IQR value:',IQR)
 print('Lower,Higher:',lower,',',higher)
 return

"""Relations between the numerical columns"""

g = sb.PairGrid(data = mydata)
g.map_diag(plt.hist)
g.map_offdiag(plt.scatter);



mydata.hist(bins=30, figsize=(15, 10))

"""##Features correlation matrix

###To have an idea of correlation between our variables we can plot the correlation matrix.
"""

plt.subplots(figsize=(15,15))
matrix = np.tril(mydata.corr())
sns.heatmap(mydata.corr(), annot = True, cmap= 'summer',square=True,mask = matrix)

"""Function definitions
Function for checking for outliers in the numeric value columns
"""

def outlier(var):
    high = np.mean(var) + 2*np.std(var)
    low = np.mean(var) - 2*np.std(var)
    outliers = (var >= high) | (var <= low)
    return outliers

"""Function to bin numerical columns"""

def bins(X, n): 
    BinWidth = (max(X) - min(X))/n
    bound1 = float('-inf')
    bound2 = min(X) + 1 * BinWidth
    bound3 = min(X) + 2 * BinWidth
    bound4 = float('inf')
    Binned = np.array([" "]*len(X)) 
    Binned[(bound1 < X) & (X <= bound2)] = 1 # Low
    Binned[(bound2 < X) & (X <= bound3)] = 2 # Med
    Binned[(bound3 < X) & (X  < bound4)] = 3 # High
    return Binned

"""Function for z-standardization of a numerical column"""

# Scaler
scalar = MinMaxScaler()

"""Remove/replace outliers for the numerical columns.
Replace outliers with median values for the numerical column, 'GENERATION'.
"""

outliers = outlier(mydata['GENERATION'])
mydata.loc[outliers, 'GENERATION'] = np.median(mydata.loc[:,"GENERATION"])

print(mydata)

"""# Data Visualization and Analysis"""

plt.figure(figsize=(10, 6))
sns.set(style = 'whitegrid')
sns.distplot(mydata['GENERATION'])
plt.title('GENERATION (Megawatthours)', fontsize = 20)
plt.xlabel('range of GENERATION (Megawatthours)')
plt.ylabel('count')

plt.figure(figsize=(10, 6))
sns.set(style = 'whitegrid')
sns.distplot(mydata['PRODUCER'])
plt.title('Type of Producer', fontsize = 20)
plt.xlabel('Range of Type of Producer')
plt.ylabel('count')
plt.text(6.5,0, "1 - Electric Generators, Electric Utilities \n2 - Combined Heat'and' Power, Industrial Power \n3 - Electric Generators, Independent Power Producers\n4 - Combined Heat 'and' Power, Commercial Power\n5 - Combined Heat 'and' Power, Electric Power")

plt.figure(figsize=(10, 6))
sns.set(style = 'whitegrid')
sns.distplot(mydata['ENERGY'])
plt.title('Source of Energy', fontsize = 20)
plt.xlabel('Range of Source of Energy')
plt.ylabel('count')
plt.text(16,0, "1 = Natural Gas \n2 = Petroleum \n3 = Coal \n4 = Other Biomass \n5 = Hydroelectric Conventional \n6 = Wood and Wood Derived Fuels \n7 = Other\n8 = Wind \n9 = Other Gasses \n10 = Nuclear \n11 = Solar Thermal and Photovoltaic \n12 = Pumped Storage  \n13 = Geothermal")

PRODUCER = mydata.PRODUCER.value_counts()
sns.set_style("darkgrid")
plt.figure(figsize=(10,4))
sns.barplot(x=PRODUCER.index, y=PRODUCER.values)
plt.text(5.5,1, "1 - Electric Generators, Electric Utilities\n2 - Combined Heat'and' Power, Industrial Power\n3 - Electric Generators, Independent Power Producers\n4 - Combined Heat 'and' Power, Commercial Power\n5 - Combined Heat 'and' Power, Electric Power\n6 - Total Electric Power Industry")
plt.show()

"""##Observing Correlation between features of the Dataset"""

correlation = mydata.corr()

plt.subplots(figsize=(30,10))
sns.heatmap( correlation, square=True, annot=True, fmt=".1f" )

X = mydata.drop(['GENERATION'],axis=1)
X.corrwith(mydata['GENERATION']).plot.bar(figsize = (20, 15), title = "Correlation with Default", 
                                        fontsize = 20,rot = 90, grid = True)

sns.jointplot(x='ENERGY',y='GENERATION',data=mydata,kind="scatter")

mydata.plot(x = 'GENERATION', y = 'MONTH', kind='scatter')

mydata.corr()

print(mydata)

"""Normalize values for the numerical columns by z-standardization

Normalize the GENERATION column; plot the normalized GENERATION data.
"""

col = mydata["GENERATION"]
mydata["GENERATION"] = norm(col)
print(mydata)

mydata.head(5)

plt.figure(figsize=(12, 8))
sns.countplot(mydata.MONTH.dropna(), order = mydata.MONTH.value_counts().index);
plt.title('Month', fontsize = 4)



plt.rcParams['figure.figsize'] = (12, 5)
# exit Generation vrs Month
sns.boxenplot(mydata['MONTH'], mydata['GENERATION'], palette = 'dark')
plt.title('Month vs Generation', fontsize = 10)
plt.xlabel('MONTH', fontsize = 8)
plt.ylabel('GENERATION', fontsize = 8)

plt.show()

plt.rcParams['figure.figsize'] = (12, 5)
# exit Generation vrs Energy
sns.boxenplot(mydata['ENERGY'], mydata['GENERATION'], palette = 'dark')
plt.title('GENERATION vs ENERGY', fontsize = 10)
plt.xlabel('ENERGY', fontsize = 8)
plt.ylabel('GENERATION', fontsize = 8)

plt.show()

plt.rcParams['figure.figsize'] = (12, 5)
# exit Generation vrs Producer
sns.boxenplot(mydata['PRODUCER'], mydata['GENERATION'], palette = 'dark')
plt.title('GENERATION vs PRODUCER', fontsize = 10)
plt.xlabel('PRODUCER', fontsize = 8)
plt.ylabel('GENERATION', fontsize = 8)
plt.text(5.5,1, "1 - Electric Generators, Electric Utilities\n2 - Combined Heat'and' Power, Industrial Power\n3 - Electric Generators, Independent Power Producers\n4 - Combined Heat 'and' Power, Commercial Power\n5 - Combined Heat 'and' Power, Electric Power\n6 - Total Electric Power Industry")
plt.show()

plt.rcParams['figure.figsize'] = (16, 5)
# exit Generation vrs Energy
sns.boxenplot(mydata['ENERGY'], mydata['GENERATION'], palette = 'dark')
plt.title('GENERATION vs ENERGY', fontsize = 10)
plt.xlabel('ENERGY', fontsize = 8)
plt.ylabel('GENERATION', fontsize = 8)
plt.text(16,0, "1 = Natural Gas \n2 = Petroleum \n3 = Coal \n4 = Other Biomass \n5 = Hydroelectric Conventional \n6 = Wood and Wood Derived Fuels \n7 = Other\n8 = Wind \n9 = Other Gasses \n10 = Nuclear \n11 = Solar Thermal and Photovoltaic \n12 = Pumped Storage  \n13 = Geothermal")
plt.show()

"""The violin plot is showing us the same pattern for each visitor type. Page Value for most of the entries are close to 0 when the Revenue is False. But the values are spread out when Revenue is True. This may come from the definition of the PageValue, which gives a value close to 0 to pages which did not generate a lot of revenue.

Transform "GENERATION" float to integer
"""

mydata['GENERATION'] = mydata['GENERATION'].astype(int)

print(mydata.info())

"""##Spiliting Dataset into training(75%) and test set(25%)"""

X = mydata.iloc[:,:-1].values
y = mydata.iloc[:,-1].values
y = y.astype('int')

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.25,random_state = 1)

X_train.shape

"""Look up the target y

"""

y

"""###Split data records"""

X_train

X_test

y_train

y_test

"""#Principal Component Analysis
We can use principal component analysis to see if we can have a good representation of our dataset in two or three dimensions.
"""

pca = PCA(n_components=3)
X_train_pca = pca.fit_transform(X_train)
per_var = np.round(pca.explained_variance_ratio_*100,decimals=1)
labels = [str(X) for x in range(1,len(per_var)+1)]
plt.rcParams['figure.figsize'] = (12, 10)
font=10
plt.bar(x=range(1,len(per_var)+1),height=per_var)
plt.tick_params(axis='x',which='both',bottom=False,top=False,labelbottom=False)
plt.ylabel('Percentage of Explained Variance', fontsize = font)
plt.xlabel('Principal Components', fontsize = font)
plt.title('Scree Plot', fontsize = font)
plt.show()

"""Because the percentage of explained variance decay slowly, it is not possible to represent well our dataset in two or three dimensions.
2D PCA Plot
We can have a look at our dataset in a 2D Plot.
"""

print(mydata.info())

"""##Building ML Model
#Overview of a few model's performance

Let's compare a few algorithms that can be used for classification to see the performance of each ones.

Applying Machine Learning Algorithm for Classification Problem
Logistic Regression
In Logistic Regression, we wish to model a dependent variable(Y) in terms of one or more independent variables(X). It is a method for classification. This algorithm is used for the dependent variable that is Categorical. Y is modeled using a function that gives output between 0 and 1 for all values of X. In Logistic Regression, the Sigmoid (aka Logistic) Function is used
"""

classifiers = {
    "Naive Bayes": GaussianNB(),
    "Logistic Regression": LogisticRegression(),
    "K Nearest Neighbour": KNeighborsClassifier(),
    "Support Vector Classification": SVC(),
    "Decision Tree Classification": DecisionTreeClassifier(),
    "Stochastic Gradient Descent": SGDClassifier(),
    "Linear Discriminant Analysis": LinearDiscriminantAnalysis(),
    "Gradient Boosting Classification ": GradientBoostingClassifier(),
    "Random Forest Classification": RandomForestClassifier()
    }

f, axes = plt.subplots(2, 5, figsize=(12, 5), sharey='row')

for i, (key, classifier) in enumerate(classifiers.items()):
    j = 0
    k = i
    if i>4:
        k = i-5
        j+=1
    
    y_pred = classifier.fit(X_train, y_train).predict(X_test)
    cf_matrix = confusion_matrix(y_test, y_pred)
    print(ConfusionMatrixDisplay)
    #print(key, "\n Accuracy:",accuracy_score(y_test,y_pred),"\n F-score",f1_score(y_test,y_pred))
    #print("GENERATION : ",mydata.GENERATION(y_test, y_pred, 
    #                                       pos_label='positive'))
    #print("MONTH : ",mydata.MONTH(y_test, y_pred, 
    #                                       pos_label='positive'))
    disp = ConfusionMatrixDisplay(cf_matrix,
                                  display_labels=["Generation","MONTH"])
    disp.plot(ax=axes[j][k], xticks_rotation=45)
    disp.ax_.set_title(key)
    disp.im_.colorbar.remove()
    disp.ax_.set_xlabel('')
    if i!=0:
        disp.ax_.set_ylabel('')

f.text(0.43, -0.1, 'Predicted label', ha='left')
plt.subplots_adjust(wspace=0.40, hspace=1)


f.colorbar(disp.im_, ax=axes)
plt.show()

"""##Bivariate analysis"""

plt.rcParams['figure.figsize'] = (15, 5)
# exit rate vs raevenue
sns.boxenplot(mydata['MONTH'], mydata['GENERATION'], palette = 'dark')
plt.title('Generation vs Month', fontsize = 10)
plt.xlabel('MONTH', fontsize = 10)
plt.ylabel('GENERATION', fontsize = 10)

plt.show()

"""## Stochastic Gradient Descent

### 
"""

from sklearn.linear_model import SGDClassifier
sgd = SGDClassifier(loss='log', penalty='l1', learning_rate='optimal',random_state=1)
sgd.fit(X_train, y_train)

y_pred = sgd.predict(X_test)
from sklearn.metrics import  accuracy_score, f1_score, precision_score, recall_score, roc_auc_score
roc=roc_auc_score(y_test, y_pred)
acc = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred)
rec = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

model = pd.DataFrame([['Stochastic Gradient Descent', acc,prec,rec, f1,roc]],
               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])
model
Model	Accuracy	Precision	Recall	F1 Score

"""##K-Nearest Neighbour"""

error_rate = []

# Will take some time
for i in range(1,40):
    
    knn = KNeighborsClassifier(n_neighbors=i,n_jobs=-1)
    knn.fit(X_train,y_train)
    pred_i = knn.predict(X_test)
    error_rate.append(np.mean(pred_i != y_test))

    plt.figure(figsize=(10,6))
plt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',
         markerfacecolor='red', markersize=10)
plt.title('Error Rate vs. K Value')
plt.xlabel('K')
plt.ylabel('Error Rate')

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=7)

knn.fit(X_train,y_train)

y_pred = knn.predict(X_test)



from sklearn.metrics import  accuracy_score, f1_score, precision_score, recall_score, roc_auc_score
roc=roc_auc_score(y_test, y_pred)
acc = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred)
rec = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

res = pd.DataFrame([['K-Nearest Neighbour', acc,prec,rec, f1,roc]],
               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])
res

"""Thus SVM tries to make a decision boundary in such a way that the separation between the two classes(that street) is as wide as possible."""

# Fitting SVM to the Training set
from sklearn.svm import SVC
scv = SVC(kernel = 'rbf', random_state = 0)
scv.fit(X_train, y_train)

# Predicting the Test set results
y_pred = scv.predict(X_test)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)

sns.heatmap(cm,annot= True,linewidths=1,cmap='coolwarm')